# EPAM-Net Standalone Backbone

çº¯PyTorchå®ç°çš„EPAM-Netä¸»å¹²ç½‘ç»œï¼Œæ— éœ€mmcvä¾èµ–ã€‚

## ğŸ“‹ ç›®å½•

- [ç®€ä»‹](#ç®€ä»‹)
- [æ¶æ„è¯´æ˜](#æ¶æ„è¯´æ˜)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†è¯´æ˜](#è¯¦ç»†è¯´æ˜)
- [æ¨¡å‹ç»“æ„](#æ¨¡å‹ç»“æ„)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸ¯ ç®€ä»‹

EPAM-Net (Efficient Pose-driven Attention-guided Multimodal Network) æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«ç½‘ç»œã€‚æœ¬ä»“åº“æä¾›äº†**ç‹¬ç«‹çš„ä¸»å¹²ç½‘ç»œå®ç°**ï¼Œæ–¹ä¾¿é›†æˆåˆ°å…¶ä»–é¡¹ç›®ä¸­ã€‚

### ä¸»è¦ç‰¹ç‚¹

- âœ… **é›¶mmcvä¾èµ–**: å®Œå…¨ä½¿ç”¨PyTorchåŸç”Ÿå®ç°
- âœ… **å³æ’å³ç”¨**: å¯ç›´æ¥æ›¿æ¢å…¶ä»–æ¨¡å‹çš„backbone
- âœ… **åŒæµæ¶æ„**: RGBæµ + éª¨æ¶å§¿æ€æµ
- âœ… **æ³¨æ„åŠ›èåˆ**: å§¿æ€ç‰¹å¾å¼•å¯¼RGBç‰¹å¾å­¦ä¹ 
- âœ… **è½»é‡é«˜æ•ˆ**: ä½¿ç”¨Temporal Shift Moduleé™ä½è®¡ç®—é‡

## ğŸ—ï¸ æ¶æ„è¯´æ˜

EPAM-Netä¸»å¹²ç½‘ç»œç”±ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼š

```
è¾“å…¥:
â”œâ”€ï¿½ï¿½ï¿½ RGBè§†é¢‘: (N, 3, 16, 224, 224)
â””â”€â”€ å§¿æ€çƒ­å›¾: (N, 17, 48, 56, 56)
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      EPAM Backbone              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RGB Stream              â”‚   â”‚
â”‚  â”‚  X3D + Temporal Shift    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“                      â”‚
â”‚      RGBç‰¹å¾                     â”‚
â”‚    (N, 432, 16, 7, 7)           â”‚
â”‚           â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Pose Stream             â”‚   â”‚
â”‚  â”‚  X3D + Temporal Shift    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“                      â”‚
â”‚      Poseç‰¹å¾                    â”‚
â”‚    (N, 216, 48, 7, 7)           â”‚
â”‚           â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Attention Module        â”‚   â”‚
â”‚  â”‚  CBAM Spatial-Temporal   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“                      â”‚
â”‚    æ³¨æ„åŠ›å›¾ (N, 1, 16, 7, 7)     â”‚
â”‚           â†“                      â”‚
â”‚    RGBç‰¹å¾ Ã— æ³¨æ„åŠ›å›¾            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
è¾“å‡º:
â”œâ”€â”€ RGBç‰¹å¾: (N, 432, 16, 7, 7)
â””â”€â”€ Poseç‰¹å¾: (N, 216, 48, 7, 7)
```

### ç»„ä»¶è¯¦è§£

#### 1. RGB Stream (X3DTemporalShift)
- **è¾“å…¥**: RGBè§†é¢‘å¸§ (N, 3, 16, 224, 224)
- **è¾“å‡º**: RGBç‰¹å¾ (N, 432, 16, 7, 7)
- **ç‰¹ç‚¹**:
  - ä½¿ç”¨X3Dé«˜æ•ˆ3D CNNæ¶æ„
  - é›†æˆTemporal Shift Moduleè¿›è¡Œæ—¶åºå»ºæ¨¡
  - SEæ¨¡å—å¢å¼ºé€šé“æ³¨æ„åŠ›

#### 2. Pose Stream (X3DTemporalShiftPose)
- **è¾“å…¥**: éª¨æ¶å§¿æ€çƒ­å›¾ (N, 17, 48, 56, 56)
  - 17ä¸ªå…³èŠ‚ç‚¹ï¼šé¼»å­ã€çœ¼ç›ã€è€³æœµã€è‚©è†€ã€æ‰‹è‚˜ã€æ‰‹è…•ã€è‡€éƒ¨ã€è†ç›–ã€è„šè¸ç­‰
- **è¾“å‡º**: Poseç‰¹å¾ (N, 216, 48, 7, 7)
- **ç‰¹ç‚¹**:
  - ä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†ç¨€ç–çš„éª¨æ¶æ•°æ®
  - æ›´å¯†é›†çš„æ—¶åºé‡‡æ ·(48å¸§ vs RGBçš„16å¸§)

#### 3. Attention Module (CBAM)
- **è¾“å…¥**: ä¸‹é‡‡æ ·çš„Poseç‰¹å¾ (N, 216, 16, 7, 7)
- **è¾“å‡º**: æ—¶ç©ºæ³¨æ„åŠ›å›¾ (N, 1, 16, 7, 7)
- **ç‰¹ç‚¹**:
  - åµŒå¥—å¼ç©ºé—´-æ—¶åºæ³¨æ„åŠ›
  - å…ˆç”Ÿæˆç©ºé—´æ³¨æ„åŠ›ï¼Œå†åœ¨å…¶åŸºç¡€ä¸Šç”Ÿæˆæ—¶åºæ³¨æ„åŠ›
  - å¼•å¯¼RGBç‰¹å¾å…³æ³¨å…³é”®å¸§å’Œæ˜¾è‘—ç©ºé—´åŒºåŸŸ

## ğŸ“¦ å®‰è£…

### ä¾èµ–è¦æ±‚

```bash
torch >= 1.7.0
torchvision >= 0.8.0
numpy
```

### å®‰è£…æ–¹æ³•

æ–¹æ³•1ï¼šç›´æ¥å¤åˆ¶æ–‡ä»¶å¤¹
```bash
# å°† epam_backbone æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ä½ çš„é¡¹ç›®ä¸­
cp -r epam_backbone /path/to/your/project/
```

æ–¹æ³•2ï¼šä½¿ç”¨ç›¸å¯¹å¯¼å…¥
```python
# åœ¨ä½ çš„ä»£ç ä¸­
import sys
sys.path.append('/path/to/EPAM-net/Multimodal-Action-Recognition-master')
from epam_backbone import EPAMBackbone
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
import torch
from epam_backbone import EPAMBackbone

# åˆ›å»ºbackbone
backbone = EPAMBackbone(
    num_classes=60,  # åŠ¨ä½œç±»åˆ«æ•°ï¼ˆå¯é€‰ï¼Œä¸å½±å“ç‰¹å¾æå–ï¼‰
    attention_type='CBAM_spatial_efficient_temporal',
    return_both_streams=True  # è¿”å›RGBå’ŒPoseä¸¤ä¸ªæµçš„ç‰¹å¾
)

# åˆå§‹åŒ–æƒé‡
backbone.init_weights()

# å‡†å¤‡è¾“å…¥æ•°æ®
rgb_videos = torch.randn(2, 3, 16, 224, 224)      # RGBè§†é¢‘
pose_heatmaps = torch.randn(2, 17, 48, 56, 56)    # å§¿æ€çƒ­å›¾

# å‰å‘ä¼ æ’­
rgb_features, pose_features = backbone(rgb_videos, pose_heatmaps)

print(f"RGBç‰¹å¾ç»´åº¦: {rgb_features.shape}")   # (2, 432, 16, 7, 7)
print(f"Poseç‰¹å¾ç»´åº¦: {pose_features.shape}") # (2, 216, 48, 7, 7)
```

### åŠ è½½é¢„è®­ç»ƒæƒé‡

```python
backbone = EPAMBackbone(
    rgb_pretrained='/path/to/rgb_pretrained.pth',
    pose_pretrained='/path/to/pose_pretrained.pth'
)
backbone.init_weights()
```

### é›†æˆåˆ°è‡ªå®šä¹‰æ¨¡å‹

```python
import torch.nn as nn

class MyActionRecognitionModel(nn.Module):
    def __init__(self, num_classes=60):
        super().__init__()

        # ä½¿ç”¨EPAM Backboneæ›¿æ¢åŸæœ‰çš„backbone
        self.backbone = EPAMBackbone(
            num_classes=num_classes,
            return_both_streams=True
        )
        self.backbone.init_weights()

        # è‡ªå®šä¹‰åˆ†ç±»å¤´
        self.rgb_classifier = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Flatten(),
            nn.Linear(432, num_classes)
        )

        self.pose_classifier = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Flatten(),
            nn.Linear(216, num_classes)
        )

    def forward(self, rgb_videos, pose_heatmaps):
        # æå–ç‰¹å¾
        rgb_feat, pose_feat = self.backbone(rgb_videos, pose_heatmaps)

        # åˆ†ç±»
        rgb_logits = self.rgb_classifier(rgb_feat)
        pose_logits = self.pose_classifier(pose_feat)

        # èåˆé¢„æµ‹
        final_logits = rgb_logits + pose_logits

        return final_logits
```

## ğŸ“ è¯¦ç»†è¯´æ˜

### è¾“å…¥æ ¼å¼

#### RGBè§†é¢‘
- **ç»´åº¦**: (N, 3, T, H, W)
- **å…¸å‹å€¼**: (N, 3, 16, 224, 224)
  - N: batch size
  - 3: RGBé€šé“
  - 16: æ—¶é—´å¸§æ•°
  - 224Ã—224: ç©ºé—´åˆ†è¾¨ç‡
- **æ•°æ®èŒƒå›´**: é€šå¸¸éœ€è¦å½’ä¸€åŒ–åˆ° [0, 1] æˆ–ä½¿ç”¨ImageNetå‡å€¼/æ–¹å·®å½’ä¸€åŒ–

#### å§¿æ€çƒ­å›¾
- **ç»´åº¦**: (N, 17, T, H, W)
- **å…¸å‹å€¼**: (N, 17, 48, 56, 56)
  - N: batch size
  - 17: éª¨æ¶å…³èŠ‚ç‚¹æ•°é‡
  - 48: æ—¶é—´å¸§æ•°ï¼ˆæ¯”RGBå¯†é›†3å€ï¼‰
  - 56Ã—56: ç©ºé—´åˆ†è¾¨ç‡
- **æ•°æ®æ ¼å¼**: é«˜æ–¯çƒ­å›¾ï¼Œæ¯ä¸ªå…³èŠ‚ç‚¹ä¸€ä¸ªé€šé“
- **å…³èŠ‚ç‚¹é¡ºåº** (COCOæ ¼å¼):
  ```
  0: é¼»å­,  1-2: çœ¼ç›,  3-4: è€³æœµ
  5-6: è‚©è†€,  7-8: æ‰‹è‚˜,  9-10: æ‰‹è…•
  11-12: è‡€éƒ¨,  13-14: è†ç›–,  15-16: è„šè¸
  ```

### è¾“å‡ºæ ¼å¼

#### RGBç‰¹å¾
- **ç»´åº¦**: (N, 432, 16, 7, 7)
  - 432: ç‰¹å¾é€šé“æ•°
  - 16: æ—¶é—´ç»´åº¦
  - 7Ã—7: ç©ºé—´ç»´åº¦ï¼ˆä»224Ã—224ä¸‹é‡‡æ ·ï¼‰

#### Poseç‰¹å¾
- **ç»´åº¦**: (N, 216, 48, 7, 7)
  - 216: ç‰¹å¾é€šé“æ•°
  - 48: æ—¶é—´ç»´åº¦
  - 7Ã—7: ç©ºé—´ç»´åº¦ï¼ˆä»56Ã—56ä¸‹é‡‡æ ·ï¼‰

### æ—¶åºå¯¹é½

RGBå’ŒPoseæµçš„æ—¶åºé•¿åº¦ä¸åŒï¼ˆ16 vs 48å¸§ï¼‰ã€‚åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼ŒPoseç‰¹å¾é€šè¿‡æ­¥é•¿ä¸º3çš„ç´¢å¼•ä¸‹é‡‡æ ·åˆ°16å¸§ï¼š

```python
time_strided_inds = [i for i in range(0, 48, 3)]  # [0, 3, 6, ..., 45]
```

è¿™æ ·å¯ä»¥è®©Poseç‰¹å¾å¼•å¯¼æ¯ä¸€å¸§RGBç‰¹å¾çš„å­¦ä¹ ã€‚

### å‚æ•°é…ç½®

```python
EPAMBackbone(
    num_classes=60,                              # åŠ¨ä½œç±»åˆ«æ•°
    rgb_pretrained=None,                          # RGBé¢„è®­ç»ƒæƒé‡è·¯å¾„
    pose_pretrained=None,                         # Poseé¢„è®­ç»ƒæƒé‡è·¯å¾„
    attention_type='CBAM_spatial_efficient_temporal',  # æ³¨æ„åŠ›ç±»å‹
    freeze_rgb=False,                             # æ˜¯å¦å†»ç»“RGB backbone
    freeze_pose=False,                            # æ˜¯å¦å†»ç»“Pose backbone
    return_both_streams=True                      # æ˜¯å¦è¿”å›ä¸¤ä¸ªæµçš„ç‰¹å¾
)
```

### æ³¨æ„åŠ›ç±»å‹

- **'CBAM_spatial_efficient_temporal'** (æ¨è): åµŒå¥—å¼ç©ºé—´-æ—¶åºæ³¨æ„åŠ›
- **'spatial_temporal'**: è”åˆç©ºé—´-æ—¶åºæ³¨æ„åŠ›

## ğŸ”§ æ¨¡å‹ç»“æ„

### å®Œæ•´æ¨¡å—åˆ—è¡¨

```
epam_backbone/
â”œâ”€â”€ __init__.py                    # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ utils.py                       # å·¥å…·å‡½æ•°ï¼ˆæ›¿ä»£mmcvï¼‰
â”œâ”€â”€ attention_module.py            # æ³¨æ„åŠ›æ¨¡å—
â”œâ”€â”€ x3d_temporal_shift_rgb.py      # RGB backbone
â”œâ”€â”€ x3d_temporal_shift_pose.py     # Pose backbone
â””â”€â”€ epam_backbone.py               # ä¸»å¹²ç½‘ç»œå°è£…
```

### å…³é”®å‚æ•°ç»Ÿè®¡

| æ¨¡å— | å‚æ•°é‡ | è®¡ç®—é‡(GFLOPs) |
|------|--------|----------------|
| RGB Stream | ~3.8M | ~6.2 |
| Pose Stream | ~1.1M | ~1.8 |
| Attention Module | ~0.01M | ~0.05 |
| **æ€»è®¡** | **~4.9M** | **~8.0** |

### ä¸åŸå§‹EPAM-Netçš„å·®å¼‚

æœ¬å®ç°ä¸è®ºæ–‡ä¸­çš„å®Œæ•´EPAM-Netçš„ä¸»è¦å·®å¼‚ï¼š

| é¡¹ç›® | å®Œæ•´EPAM-Net | Standalone Backbone |
|------|--------------|---------------------|
| åˆ†ç±»å¤´ | âœ… åŒ…å«I3D Head | âŒ ä¸åŒ…å« |
| æœ€ç»ˆé¢„æµ‹ | âœ… è¾“å‡ºlogits | âŒ è¾“å‡ºç‰¹å¾ |
| è®­ç»ƒLoss | âœ… åŒæµç›‘ç£ | âŒ æ— Loss |
| ç”¨é€” | ç«¯åˆ°ç«¯è®­ç»ƒ | ç‰¹å¾æå– |

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. å†…å­˜ä¼˜åŒ–

å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š

```python
# ä½¿ç”¨æ›´å°çš„batch size
batch_size = 4  # é™ä½batch size

# æˆ–å†»ç»“æŸä¸ªbackbone
backbone = EPAMBackbone(freeze_pose=True)  # å†»ç»“Poseæµ
```

### 2. ç‰¹å¾æå–

åªéœ€è¦æŸä¸€ä¸ªæµçš„ç‰¹å¾ï¼š

```python
# åªæå–RGBç‰¹å¾ï¼ˆä½†ä»éœ€è¦Poseè¾“å…¥ï¼‰
rgb_feat, _ = backbone(rgb_videos, pose_heatmaps)

# æˆ–ç›´æ¥è®¿é—®å­æ¨¡å—
rgb_feat = backbone.rgb_backbone(rgb_videos)
pose_feat = backbone.pose_backbone(pose_heatmaps)
```

### 3. å¾®è°ƒç­–ç•¥

```python
# å†»ç»“backboneï¼Œåªè®­ç»ƒåˆ†ç±»å¤´
backbone = EPAMBackbone(freeze_rgb=True, freeze_pose=True)

# æˆ–ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
optimizer = torch.optim.SGD([
    {'params': backbone.rgb_backbone.parameters(), 'lr': 1e-4},
    {'params': backbone.pose_backbone.parameters(), 'lr': 1e-4},
    {'params': classifier.parameters(), 'lr': 1e-3}  # åˆ†ç±»å¤´ç”¨æ›´å¤§å­¦ä¹ ç‡
])
```

### 4. æ•°æ®é¢„å¤„ç†

#### RGBè§†é¢‘é¢„å¤„ç†
```python
import torchvision.transforms as transforms

rgb_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])
```

#### å§¿æ€çƒ­å›¾ç”Ÿæˆ
```python
import numpy as np

def generate_pose_heatmap(keypoints, img_size=56, sigma=0.6):
    """
    ä»å…³èŠ‚ç‚¹åæ ‡ç”Ÿæˆé«˜æ–¯çƒ­å›¾

    Args:
        keypoints: (T, 17, 3) - Tå¸§ï¼Œ17ä¸ªå…³èŠ‚ï¼Œ(x, y, score)
        img_size: çƒ­å›¾å°ºå¯¸
        sigma: é«˜æ–¯æ ¸æ ‡å‡†å·®

    Returns:
        heatmap: (17, T, img_size, img_size)
    """
    T, num_joints, _ = keypoints.shape
    heatmap = np.zeros((num_joints, T, img_size, img_size))

    for t in range(T):
        for j in range(num_joints):
            x, y, score = keypoints[t, j]
            if score > 0:
                # ç”Ÿæˆé«˜æ–¯çƒ­å›¾
                # ... (å…·ä½“å®ç°è§data_preparationè„šæœ¬)
                pass

    return heatmap
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†ä¸åŒé•¿åº¦çš„è§†é¢‘ï¼Ÿ

**A**: ä½¿ç”¨é‡‡æ ·ç­–ç•¥ï¼š
```python
# å‡åŒ€é‡‡æ ·16å¸§ç”¨äºRGB
def uniform_sample(video, num_frames=16):
    total_frames = len(video)
    indices = np.linspace(0, total_frames-1, num_frames).astype(int)
    return video[indices]
```

### Q2: å¯ä»¥åªä½¿ç”¨RGBæµå—ï¼Ÿ

**A**: æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†æ³¨æ„åŠ›æ¨¡å—éœ€è¦Poseç‰¹å¾ä½œä¸ºè¾“å…¥ã€‚å¦‚æœåªæƒ³ç”¨RGBï¼š
```python
# ç›´æ¥ä½¿ç”¨RGB backbone
rgb_backbone = X3DTemporalShift()
rgb_feat = rgb_backbone(rgb_videos)
```

### Q3: è¾“å…¥å°ºå¯¸å¯ä»¥æ”¹å˜å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†éœ€è¦ç›¸åº”è°ƒæ•´ï¼š
- RGB: å¯ä»¥ä½¿ç”¨å…¶ä»–åˆ†è¾¨ç‡(å¦‚112x112)ï¼Œç‰¹å¾å›¾å°ºå¯¸ä¼šç›¸åº”å˜åŒ–
- Pose: å»ºè®®ä¿æŒ56x56ï¼Œå› ä¸ºå§¿æ€æ•°æ®æœ¬èº«åˆ†è¾¨ç‡ä¸é«˜

### Q4: å¦‚ä½•å¯è§†åŒ–æ³¨æ„åŠ›å›¾ï¼Ÿ

**A**: å¯ä»¥åœ¨forwardä¸­è¿”å›attention_mapsï¼š
```python
# ä¿®æ”¹EPAMBackbone.forward
def forward(self, rgb_videos, pose_heatmaps, return_attention=False):
    # ...
    attention_maps = self.attention_module(time_strided_pose_feats)

    if return_attention:
        return rgb_fused, pose_feats, attention_maps
    return rgb_fused, pose_feats
```

### Q5: æŠ¥é”™"RuntimeError: CUDA out of memory"æ€ä¹ˆåŠï¼Ÿ

**A**:
1. å‡å°batch size
2. ä½¿ç”¨gradient checkpointing
3. å†»ç»“éƒ¨åˆ†backbone
4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP)

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    features = backbone(rgb, pose)
    loss = criterion(features, labels)
scaler.scale(loss).backward()
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

åœ¨NTU RGB+D 60æ•°æ®é›†ä¸Šçš„ç‰¹å¾æå–é€Ÿåº¦ï¼ˆå•GPU RTX 3090ï¼‰ï¼š

| Batch Size | ååé‡ (videos/sec) | GPUå†…å­˜ |
|-----------|---------------------|---------|
| 1 | 12.5 | 2.1 GB |
| 4 | 38.2 | 6.8 GB |
| 8 | 62.4 | 12.5 GB |
| 16 | 89.7 | 22.1 GB |

## ğŸ“„ å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨åŸå§‹è®ºæ–‡ï¼š

```bibtex
@article{abdelkawy2025epam,
  title={EPAM-Net: An efficient pose-driven attention-guided multimodal network for video action recognition},
  author={Abdelkawy, Ahmed and Ali, Asem and Farag, Aly},
  journal={Neurocomputing},
  pages={129781},
  year={2025},
  publisher={Elsevier}
}
```

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æœ¬READMEçš„å¸¸è§é—®é¢˜éƒ¨åˆ†
2. æŸ¥çœ‹ä¸»é¡¹ç›®çš„CLAUDE.mdæ–‡æ¡£
3. æäº¤Issueåˆ°GitHubä»“åº“

## ğŸ“œ è®¸å¯è¯

æœ¬ä»£ç éµå¾ªåŸé¡¹ç›®çš„è®¸å¯è¯ã€‚
