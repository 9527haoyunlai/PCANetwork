"""
EPAM Backbone æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import torch
import sys

def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯•1: æ¨¡å—å¯¼å…¥")
    print("=" * 60)

    try:
        from epam_backbone import EPAMBackbone
        print("âœ“ EPAMBackbone å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— EPAMBackbone å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        from epam_backbone import X3DTemporalShift
        print("âœ“ X3DTemporalShift å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— X3DTemporalShift å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        from epam_backbone import X3DTemporalShiftPose
        print("âœ“ X3DTemporalShiftPose å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— X3DTemporalShiftPose å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        from epam_backbone import CBAMSpatialEfficientTemporalAttention
        print("âœ“ CBAMSpatialEfficientTemporalAttention å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— CBAMSpatialEfficientTemporalAttention å¯¼å…¥å¤±è´¥: {e}")
        return False

    print("\nâœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æµ‹è¯•é€šè¿‡!\n")
    return True


def test_rgb_backbone():
    """æµ‹è¯•RGB backbone"""
    print("=" * 60)
    print("æµ‹è¯•2: RGB Backbone")
    print("=" * 60)

    try:
        from epam_backbone import X3DTemporalShift

        print("\nåˆ›å»ºRGB Backbone...")
        model = X3DTemporalShift(
            gamma_w=1,
            gamma_b=2.25,
            gamma_d=2.2,
            se_style='half'
        )
        model.init_weights()
        model.eval()

        print("è¾“å…¥æ•°æ®...")
        x = torch.randn(1, 3, 16, 224, 224)
        print(f"  è¾“å…¥ç»´åº¦: {x.shape}")

        print("å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = model(x)

        print(f"  è¾“å‡ºç»´åº¦: {output.shape}")
        print(f"  ç‰¹å¾é€šé“æ•°: {model.feat_dim}")

        assert output.shape == (1, 432, 16, 7, 7), f"è¾“å‡ºç»´åº¦é”™è¯¯: {output.shape}"

        print("\nâœ“ RGB Backboneæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— RGB Backboneæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_pose_backbone():
    """æµ‹è¯•Pose backbone"""
    print("=" * 60)
    print("æµ‹è¯•3: Pose Backbone")
    print("=" * 60)

    try:
        from epam_backbone import X3DTemporalShiftPose

        print("\nåˆ›å»ºPose Backbone...")
        model = X3DTemporalShiftPose(
            gamma_d=1,
            in_channels=17,
            base_channels=24,
            num_stages=3,
            stage_blocks=(5, 11, 7),
            spatial_strides=(2, 2, 2),
            conv1_stride=1
        )
        model.init_weights()
        model.eval()

        print("è¾“å…¥æ•°æ®...")
        x = torch.randn(1, 17, 48, 56, 56)
        print(f"  è¾“å…¥ç»´åº¦: {x.shape}")

        print("å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = model(x)

        print(f"  è¾“å‡ºç»´åº¦: {output.shape}")
        print(f"  ç‰¹å¾é€šé“æ•°: {model.feat_dim}")

        assert output.shape == (1, 216, 48, 7, 7), f"è¾“å‡ºç»´åº¦é”™è¯¯: {output.shape}"

        print("\nâœ“ Pose Backboneæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— Pose Backboneæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_attention_module():
    """æµ‹è¯•æ³¨æ„åŠ›æ¨¡å—"""
    print("=" * 60)
    print("æµ‹è¯•4: Attention Module")
    print("=" * 60)

    try:
        from epam_backbone import CBAMSpatialEfficientTemporalAttention

        print("\nåˆ›å»ºAttention Module...")
        attention = CBAMSpatialEfficientTemporalAttention(attention_type='nested')

        print("è¾“å…¥æ•°æ®...")
        x = torch.randn(2, 216, 16, 7, 7)
        print(f"  è¾“å…¥ç»´åº¦: {x.shape}")

        print("å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = attention(x)

        print(f"  è¾“å‡ºç»´åº¦: {output.shape}")

        assert output.shape == (2, 1, 16, 7, 7), f"è¾“å‡ºç»´åº¦é”™è¯¯: {output.shape}"

        print("\nâœ“ Attention Moduleæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— Attention Moduleæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_epam_backbone():
    """æµ‹è¯•å®Œæ•´EPAM Backbone"""
    print("=" * 60)
    print("æµ‹è¯•5: å®Œæ•´EPAM Backbone")
    print("=" * 60)

    try:
        from epam_backbone import EPAMBackbone

        print("\nåˆ›å»ºEPAM Backbone...")
        backbone = EPAMBackbone(
            num_classes=60,
            attention_type='CBAM_spatial_efficient_temporal',
            return_both_streams=True
        )
        backbone.init_weights()
        backbone.eval()

        print("è¾“å…¥æ•°æ®...")
        rgb_videos = torch.randn(2, 3, 16, 224, 224)
        pose_heatmaps = torch.randn(2, 17, 48, 56, 56)
        print(f"  RGBè¾“å…¥: {rgb_videos.shape}")
        print(f"  Poseè¾“å…¥: {pose_heatmaps.shape}")

        print("å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            rgb_feat, pose_feat = backbone(rgb_videos, pose_heatmaps)

        print(f"  RGBç‰¹å¾: {rgb_feat.shape}")
        print(f"  Poseç‰¹å¾: {pose_feat.shape}")

        assert rgb_feat.shape == (2, 432, 16, 7, 7), f"RGBç‰¹å¾ç»´åº¦é”™è¯¯: {rgb_feat.shape}"
        assert pose_feat.shape == (2, 216, 48, 7, 7), f"Poseç‰¹å¾ç»´åº¦é”™è¯¯: {pose_feat.shape}"

        print("\næµ‹è¯•ç‰¹å¾ç»´åº¦ä¿¡æ¯...")
        dims = backbone.get_feature_dims()
        print(f"  RGBé€šé“: {dims['rgb_channels']}")
        print(f"  Poseé€šé“: {dims['pose_channels']}")

        print("\nâœ“ å®Œæ•´EPAM Backboneæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— å®Œæ•´EPAM Backboneæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("=" * 60)
    print("æµ‹è¯•6: æ¢¯åº¦æµ")
    print("=" * 60)

    try:
        from epam_backbone import EPAMBackbone

        print("\nåˆ›å»ºEPAM Backbone...")
        backbone = EPAMBackbone(return_both_streams=False)
        backbone.init_weights()
        backbone.train()

        print("è¾“å…¥æ•°æ®...")
        rgb_videos = torch.randn(2, 3, 16, 224, 224, requires_grad=True)
        pose_heatmaps = torch.randn(2, 17, 48, 56, 56, requires_grad=True)

        print("å‰å‘ä¼ æ’­...")
        features = backbone(rgb_videos, pose_heatmaps)

        print("è®¡ç®—æŸå¤±...")
        loss = features.mean()

        print("åå‘ä¼ æ’­...")
        loss.backward()

        print("æ£€æŸ¥æ¢¯åº¦...")
        has_grad = rgb_videos.grad is not None
        print(f"  è¾“å…¥æ˜¯å¦æœ‰æ¢¯åº¦: {has_grad}")

        if has_grad:
            print(f"  æ¢¯åº¦èŒƒæ•°: {rgb_videos.grad.norm().item():.6f}")

        print("\nâœ“ æ¢¯åº¦æµæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_freeze():
    """æµ‹è¯•å†»ç»“åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•7: Backboneå†»ç»“")
    print("=" * 60)

    try:
        from epam_backbone import EPAMBackbone

        print("\nåˆ›å»ºå†»ç»“çš„Backbone...")
        backbone = EPAMBackbone(freeze_rgb=True, freeze_pose=True)
        backbone.init_weights()

        print("æ£€æŸ¥å‚æ•°...")
        rgb_trainable = sum(p.numel() for p in backbone.rgb_backbone.parameters() if p.requires_grad)
        pose_trainable = sum(p.numel() for p in backbone.pose_backbone.parameters() if p.requires_grad)

        print(f"  RGBå¯è®­ç»ƒå‚æ•°: {rgb_trainable}")
        print(f"  Poseå¯è®­ç»ƒå‚æ•°: {pose_trainable}")

        assert rgb_trainable == 0, "RGB backboneæœªæ­£ç¡®å†»ç»“"
        assert pose_trainable == 0, "Pose backboneæœªæ­£ç¡®å†»ç»“"

        print("\nâœ“ Backboneå†»ç»“æµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— Backboneå†»ç»“æµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_batch_sizes():
    """æµ‹è¯•ä¸åŒbatch size"""
    print("=" * 60)
    print("æµ‹è¯•8: ä¸åŒBatch Size")
    print("=" * 60)

    try:
        from epam_backbone import EPAMBackbone

        backbone = EPAMBackbone()
        backbone.init_weights()
        backbone.eval()

        batch_sizes = [1, 2, 4, 8]

        for bs in batch_sizes:
            print(f"\næµ‹è¯•batch size = {bs}...")
            rgb_videos = torch.randn(bs, 3, 16, 224, 224)
            pose_heatmaps = torch.randn(bs, 17, 48, 56, 56)

            with torch.no_grad():
                rgb_feat, pose_feat = backbone(rgb_videos, pose_heatmaps)

            assert rgb_feat.shape[0] == bs, f"RGBç‰¹å¾batch sizeé”™è¯¯"
            assert pose_feat.shape[0] == bs, f"Poseç‰¹å¾batch sizeé”™è¯¯"
            print(f"  âœ“ batch size {bs} é€šè¿‡")

        print("\nâœ“ ä¸åŒBatch Sizeæµ‹è¯•é€šè¿‡!\n")
        return True

    except Exception as e:
        print(f"\nâœ— ä¸åŒBatch Sizeæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("EPAM Backbone å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 60 + "\n")

    results = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    results.append(("RGB Backbone", test_rgb_backbone()))
    results.append(("Pose Backbone", test_pose_backbone()))
    results.append(("Attention Module", test_attention_module()))
    results.append(("å®Œæ•´EPAM Backbone", test_epam_backbone()))
    results.append(("æ¢¯åº¦æµ", test_gradient_flow()))
    results.append(("Backboneå†»ç»“", test_freeze()))
    results.append(("ä¸åŒBatch Size", test_batch_sizes()))

    # è¾“å‡ºæ€»ç»“
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{name:.<40} {status}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == '__main__':
    sys.exit(main())
